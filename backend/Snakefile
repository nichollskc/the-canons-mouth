configfile: "config.json"

rule download_file:
    output:
        out="texts/raw/{name}.{ext}",
    params:
        url=lambda wildcards: config['texts_urls'][wildcards.name]['urls'][wildcards.ext]
    shell:
        "wget -O {output.out} {params.url}"

rule txt_to_jsx:
    input:
        txt="texts/raw/{name}.txt",
    output:
        js="../frontend/src/texts/{name}.js"
    run:
        import process_texts as pt
        pt.txt_to_text_export(wildcards.name,
                              input.txt,
                              output.js)

rule txt_to_searchable:
    input:
        txt="texts/raw/{name}.txt",
    output:
        txt="texts/search/{name}.txt",
        chapters="texts/chapters/{name}.json",
    run:
        import process_texts as pt
        pt.process_txt_for_search(input.txt, output.txt, output.chapters, wildcards.name)

rule txt_to_html:
    input:
        txt="texts/search/{name}.txt",
        template="texts/text_template.html",
        chapters="texts/chapters/{name}.json",
    output:
        html="../frontend/public/texts/{name}.html",
    run:
        import process_texts as pt
        pt.make_html_with_anchors_all_chapters(input.txt,
                                               config['texts'][wildcards.name]['full_name'],
                                               input.template,
                                               output.html,
                                               input.chapters)

rule txt_trim:
    input:
        txt="texts/tokenized/{name}.txt",
    output:
        txt="texts/small/tokenized/{name}.txt",
    shell:
        "head -n 500 {input.txt} > {output.txt}"

rule add_sentence_breaks_chapters:
    input:
        txt="texts/search/{name}.txt",
    output:
        chapters="texts/chapter_starts/{name}.json",
        txt="texts/tokenized/{name}.txt",
    run:
        import process_texts as pt
        lines = pt.extract_chapter_markers(input.txt, output.chapters)
        pt.add_sentence_breaks(''.join(lines), output.txt)

rule text_props_js:
    output:
        js="../frontend/src/texts.js"
    run:
        import process_texts as pt
        pt.write_text_prop_dict_js(config, output.js)

rule all_js:
    input:
        expand("../frontend/src/texts/{name}.js",
               name=config['texts'].keys()),
        texts="../frontend/src/texts.js",

rule update_texts:
    input:
        expand("texts/raw/{name}.html",
               name=config['texts_urls'].keys()),
        expand("texts/{folder}/{name}.txt",
               folder=['search', 'raw'],
               name=config['texts'].keys()),
        expand("../frontend/public/texts/{name}.html",
               name=config['texts'].keys()),
        js="../frontend/src/texts.js"
